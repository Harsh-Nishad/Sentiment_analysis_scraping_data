{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HARSH\\Anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "Using state Tamil Nadu server backend.\n"
     ]
    }
   ],
   "source": [
    "# %pip install certifi --ignore-installed\n",
    "# %pip install selenium\n",
    "# %pip install langdetect\n",
    "# %pip install emoji\n",
    "# %pip install translators\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "from emoji import EMOJI_DATA\n",
    "\n",
    "\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import translators as ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> regex for twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "# stopwords = open('./stopwords.txt', 'r').read().split(\"\\n\")\n",
    "\n",
    "def regex_clean(text):\n",
    "    clean_text = \"\"\n",
    "\n",
    "    # Changing original to lowercase\n",
    "    text.lower()\n",
    "    \n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "    text = re.sub('#', '', text)\n",
    "    # Replacing HTML tags\n",
    "    # Tokenization and removal of few special characters\n",
    "    clean_text = re.sub(\"<[a-zA-Z]*>|</[a-zA-Z]*>|[\\W+]|nbsp\", \" \", text).split(\" \")\n",
    "\n",
    "    removed_words = \"\"\n",
    "    for word in clean_text:\n",
    "        # stopword condition removed for better grammar in skill-removal/tag-filter function\n",
    "        if not word in stopwords:\n",
    "        # Removing the empty strings\n",
    "            if word!='':\n",
    "                removed_words += word + \" \"\n",
    "\n",
    "\n",
    "\n",
    "    return removed_words.lower()\n",
    "   # print(clean_text)\n",
    "    return removed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "import csv\n",
    "\n",
    "\n",
    "def scraptweets(search_words, date_since, numTweets, numRuns):\n",
    "\n",
    "    # Define a pandas dataframe to store the data:\n",
    "    db_tweets = pd.DataFrame(columns = ['username', 'text'])\n",
    "\n",
    "    for i in range(0, numRuns):\n",
    "        \n",
    "        # Collect tweets using the Cursor object\n",
    "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
    "        # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
    "        tweets = tweepy.Cursor(api.search_tweets, q=search_words, since_id=date_since, count=numTweets).items()\n",
    "        \n",
    "        # Store these tweets into a python list\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "        \n",
    "        # Obtain the following info (methods to call them out):\n",
    "            # user.screen_name - twitter handle\n",
    "            # user.description - description of account\n",
    "            # user.location - where is he tweeting from\n",
    "            # user.friends_count - no. of other users that user is following (following)\n",
    "            # user.followers_count - no. of other users who are following this user (followers)\n",
    "            # user.statuses_count - total tweets by user\n",
    "            # user.created_at - when the user account was created\n",
    "            # created_at - when the tweet was created\n",
    "            # retweet_count - no. of retweets\n",
    "            # (deprecated) user.favourites_count - probably total no. of tweets that is favourited by user\n",
    "            # retweeted_status.full_text - full text of the tweet\n",
    "            # tweet.entities['hashtags'] - hashtags in the tweet\n",
    "        \n",
    "        # Begin scraping the tweets individually:\n",
    "        noTweets = 0\n",
    "        with open('data1.csv', 'a') as csvfile:\n",
    "            filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)  # Opening filewriter to append as csv\n",
    "            for tweet in tweet_list:\n",
    "        # Pull the values\n",
    "                username = tweet.user.screen_name\n",
    "                try:\n",
    "                    text = tweet.text\n",
    "                except AttributeError:  # Not a Retweet\n",
    "                    text = tweet.full_text\n",
    "                # Add the variables to the empty list - ith_tweet:\n",
    "                ith_tweet = [regex_clean(text), 'twitter']\n",
    "                # Append to dataframe - db_tweets\n",
    "                db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "                # increase counter - noTweets  \n",
    "                noTweets += 1\n",
    "                # print(\"____NEW____\", noTweets)\n",
    "            #  print(tweet)\n",
    "                # Writing the cleaned text to the csv file\n",
    "                try:\n",
    "                    filewriter.writerow(ith_tweet)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "\n",
    "    # path = os.getcwd()\n",
    "    # filename = path + '\\data1.csv'\n",
    "    # db_tweets.to_csv(filename, index = False)\n",
    "\n",
    "    # Printing the total number of tweets scraped\n",
    "    print(noTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %pip install tweepy\n",
    "import tweepy as tw\n",
    "\n",
    "\n",
    "API_KEY = 'R5h6M3ZNhlNkNmNeZVwgWfo71'\n",
    "SECRET_KEY = 'E7buuYj70YvEB3CfQnMv0t5mKUGrSra866pbvO9rlfWAFHuVGy'\n",
    "BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAAHPohAEAAAAAyVIP07bELy86Bv6bWENRQUmnOgg%3DqfXlW1t9NsNUjiRNjBAV4SkOdvcvcaW2UX3diFI9IBh5deTmZr'\n",
    "ACCESS_TOKEN = '4881363287-NFZJvL4NxSyq9PX2f4O0q740AHhSLhnTnKy3TEl'\n",
    "SECRET_TOKEN = 'm33cJHTRSkLkEHpJpmbUdj0Dhwe5fOloZlkbWSQbNU0lq'\n",
    "\n",
    "auth = tw.OAuthHandler(API_KEY, SECRET_KEY)\n",
    "auth.set_access_token(ACCESS_TOKEN, SECRET_TOKEN)\n",
    "api = tw.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Initialise these variables:\n",
    "import tweepy as tw\n",
    "search_words = \"IIT \"\n",
    "date_since = -1\n",
    "numTweets = 2500\n",
    "numRuns = 1\n",
    "\n",
    "# Call the function scraptweets\n",
    "scraptweets(search_words, date_since, 2500, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sched, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise these variables:\n",
    "import tweepy as tw\n",
    "search_words = \"indian institute of technology OR iit OR IIT OR IITB OR IIT DELHI OR IIT MUMBAI OR IIT MADRAS OR DEMERITS OF IIT OR Sucide IIT \"\n",
    "date_since = -1\n",
    "numTweets = 2500\n",
    "numRuns = 1\n",
    "\n",
    "import sched, time\n",
    "s = sched.scheduler(time.time, time.sleep)\n",
    "def do_something(sc): \n",
    "    print(\"beware intiallizing...\")\n",
    "    scraptweets(search_words, date_since, 2500, 1)\n",
    "    sc.enter(900, 1, do_something, (sc,))\n",
    "\n",
    "s.enter(900, 1, do_something, (s,))\n",
    "s.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rt dt_next indian institute technology madras iit madras researchers shown spike protein vaccines may effective</th>\n",
       "      <th>twitter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt ians_india a team researchers indian instit...</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt dhfwka spike protein vaccines may effective...</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rt dhfwka spike protein vaccines may effective...</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt ians_india a team researchers indian instit...</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt dhfwka spike protein vaccines may effective...</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>rt drsubhassarkar visited atal bihari vajpayee...</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2889</th>\n",
       "      <td>rt ram_bhaktha permanent campus indian institu...</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2890</th>\n",
       "      <td>rt darshanajardosh participated annual convoca...</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2891</th>\n",
       "      <td>rt darshanajardosh participated annual convoca...</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2892</th>\n",
       "      <td>rt neeksww i happy share i joined indian insti...</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2893 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rt dt_next indian institute technology madras iit madras researchers shown spike protein vaccines may effective   \\\n",
       "0     rt ians_india a team researchers indian instit...                                                                 \n",
       "1     rt dhfwka spike protein vaccines may effective...                                                                 \n",
       "2     rt dhfwka spike protein vaccines may effective...                                                                 \n",
       "3     rt ians_india a team researchers indian instit...                                                                 \n",
       "4     rt dhfwka spike protein vaccines may effective...                                                                 \n",
       "...                                                 ...                                                                 \n",
       "2888  rt drsubhassarkar visited atal bihari vajpayee...                                                                 \n",
       "2889  rt ram_bhaktha permanent campus indian institu...                                                                 \n",
       "2890  rt darshanajardosh participated annual convoca...                                                                 \n",
       "2891  rt darshanajardosh participated annual convoca...                                                                 \n",
       "2892  rt neeksww i happy share i joined indian insti...                                                                 \n",
       "\n",
       "      twitter  \n",
       "0     twitter  \n",
       "1     twitter  \n",
       "2     twitter  \n",
       "3     twitter  \n",
       "4     twitter  \n",
       "...       ...  \n",
       "2888  twitter  \n",
       "2889  twitter  \n",
       "2890  twitter  \n",
       "2891  twitter  \n",
       "2892  twitter  \n",
       "\n",
       "[2893 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "filename = path + '\\data1.csv'\n",
    "csv2 = pd.read_csv(filename)\n",
    "try_df = pd.DataFrame(csv2)\n",
    "try_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleantext import clean\n",
    "import csv\n",
    "from pickle import UNICODE\n",
    "# from emoji import UNICODE_EMOJI\n",
    "from langdetect import detect\n",
    "\n",
    "with open('data1.csv', 'a') as csvfile:\n",
    "    filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)  # Opening filewriter to append as csv\n",
    "\n",
    "    for index, row in try_df.iterrows():\n",
    "\n",
    "        # Translating the text to english if required\n",
    "        current_lang = detect(row['text'])\n",
    "        translated_text = row['text'] \n",
    "        # print(translated_text)\n",
    "        \n",
    "        # for char in translated_text:\n",
    "        #     if char in UNICODE_EMOJI['en']:\n",
    "        #         re.sub(char, '', translated_text)\n",
    "        translated_text= clean(translated_text, no_emoji=True)\n",
    "        #print(translated_text)\n",
    "\n",
    "        if not current_lang==\"en\":\n",
    "             translated_text = ts.google(row['text'], from_language=current_lang, to_language='en')\n",
    "                # Cleaning the translated text\n",
    "             clean_text = regex_clean(translated_text)\n",
    "        \n",
    "        # Writing the cleaned text to the csv file\n",
    "        try:\n",
    "            filewriter.writerow([''.join(clean_text), 'twitter'])\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "filename = path + '\\data1.csv'\n",
    "filename\n",
    "csv = pd.read_csv(filename)\n",
    "csv\n",
    "try_df = pd.DataFrame(csv)\n",
    "try_df\n",
    "try_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "299efe7f231e8caa091840d38116dea3bca084be79af818ea1f79aa34bf0e418"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
